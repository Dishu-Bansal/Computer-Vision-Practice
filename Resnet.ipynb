{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class identity_block(nn.Module):\n",
    "    def __init__(self, in_fts, kernel_size, filters):\n",
    "        super(identity_block, self).__init__()\n",
    "        self.f1, self.f2, self.f3 = filters\n",
    "        self.l1 = nn.Conv2d(in_fts, self.f1, kernel_size=(1,1), padding='valid')\n",
    "        self.l2 = nn.BatchNorm2d(self.f1)\n",
    "        self.l3 = nn.ReLU()\n",
    "        self.l4 = nn.Conv2d(self.f1, self.f2, kernel_size=kernel_size, padding='same')\n",
    "        self.l5 = nn.BatchNorm2d(self.f2)\n",
    "        self.l6 = nn.ReLU()\n",
    "        self.l7 = nn.Conv2d(self.f2, self.f3, kernel_size=(1,1), padding='valid')\n",
    "        self.l8 = nn.BatchNorm2d(self.f3)\n",
    "    \n",
    "    def forward(self, input_):\n",
    "        x = self.l1(input_)\n",
    "        x = self.l2(x)\n",
    "        x = self.l3(x)\n",
    "\n",
    "        x = self.l4(x)\n",
    "        x = self.l5(x)\n",
    "        x = self.l6(x)\n",
    "\n",
    "        x = self.l7(x)\n",
    "        x = self.l8(x)\n",
    "\n",
    "        x = x + input_\n",
    "        x = nn.ReLU()(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class conv_block(nn.Module):\n",
    "    def __init__(self, in_fts, kernel_size, filters, strides=(2,2)):\n",
    "        super(conv_block, self).__init__()\n",
    "        self.f1, self.f2, self.f3 = filters\n",
    "        self.l1 = nn.Conv2d(in_fts, self.f1, kernel_size=(1,1), stride=strides)\n",
    "        self.l2 = nn.BatchNorm2d(self.f1)\n",
    "        self.l3 = nn.ReLU()\n",
    "        self.l4 = nn.Conv2d(self.f1, self.f2, kernel_size=kernel_size, padding='same')\n",
    "        self.l5 = nn.BatchNorm2d(self.f2)\n",
    "        self.l6 = nn.ReLU()\n",
    "        self.l7 = nn.Conv2d(self.f2, self.f3, kernel_size=(1,1))\n",
    "        self.l8 = nn.BatchNorm2d(self.f3)\n",
    "        self.l9 = nn.Conv2d(in_fts, self.f3, kernel_size=(1,1), stride=strides)\n",
    "    \n",
    "    def forward(self, input_):\n",
    "        x = self.l1(input_)\n",
    "        x = self.l2(x)\n",
    "        x = self.l3(x)\n",
    "\n",
    "        x = self.l4(x)\n",
    "        x = self.l5(x)\n",
    "        x = self.l6(x)\n",
    "\n",
    "        x = self.l7(x)\n",
    "        x = self.l8(x)\n",
    "\n",
    "        shortcut = self.l9(input_)\n",
    "\n",
    "        x = x + shortcut\n",
    "        x = nn.ReLU()(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyResnet(nn.Module):\n",
    "    def __init__(self, K):\n",
    "        super(MyResnet, self).__init__()\n",
    "        self.l1 = nn.ZeroPad2d(padding=(3,3))\n",
    "        self.l2 = nn.Conv2d(3, 64, kernel_size=(7,7), stride=(2,2), padding='valid')\n",
    "        self.l3 = nn.BatchNorm2d(64)\n",
    "        self.l4 = nn.ZeroPad2d(padding=(1,1))\n",
    "        self.l5 = nn.MaxPool2d(kernel_size=(3,3), stride=(2,2))\n",
    "        self.l6 = nn.Sequential(\n",
    "            conv_block(64, 3, [64, 64, 256], strides=(1,1)),\n",
    "            identity_block(256, 3, [64, 64, 256]),\n",
    "            identity_block(256, 3, [64, 64, 256])\n",
    "        )\n",
    "        self.l7 = nn.Sequential(\n",
    "            conv_block(256, 3, [128, 128, 512]),\n",
    "            identity_block(512, 3, [128, 128, 512]),\n",
    "            identity_block(512, 3, [128, 128, 512]),\n",
    "            identity_block(512, 3, [128, 128, 512])\n",
    "        )\n",
    "        self.l8 = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.l9 = nn.Linear(512, 1024)\n",
    "        #self.l92 = nn.Linear(256, 128)\n",
    "        self.l92 = nn.Dropout(0.2)\n",
    "        self.l93 = nn.Linear(1024,3)\n",
    "        self.l10 = nn.Linear(512, 1024)\n",
    "        #self.l11 = nn.Linear(256, 128)\n",
    "        #self.l12 = nn.Linear(128, 64)\n",
    "        self.l11 = nn.Dropout(0.3)\n",
    "        self.l13 = nn.Linear(1024, 4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.l1(x)\n",
    "        x = self.l2(x)\n",
    "        x = self.l3(x)\n",
    "        x = nn.ReLU()(x)\n",
    "        x = self.l4(x)\n",
    "        x = self.l5(x)\n",
    "\n",
    "        x = self.l6(x)\n",
    "\n",
    "        x = self.l7(x)\n",
    "\n",
    "        x = self.l8(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x1 = self.l9(x)\n",
    "        x1 = nn.ReLU()(x1)\n",
    "        x1 = self.l92(x1)\n",
    "        x1 = self.l93(x1)\n",
    "        # x1 = nn.Softmax(dim=1)(x1)\n",
    "        x2 = self.l10(x)\n",
    "        x2 = nn.ReLU()(x2)\n",
    "        x2 = self.l11(x2)\n",
    "        #x2 = self.l12(x2)\n",
    "        x2 = self.l13(x2)\n",
    "        x2 = nn.Sigmoid()(x2)\n",
    "        return torch.concatenate((x1,x2), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('FaceMask/output.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "sizes = { d['file_name']: (d['height'], d['width']) for d in data['images'] }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "new_height = 128\n",
    "new_width = 128\n",
    "mydata = {}\n",
    "for i in data['annotations']:\n",
    "    name = f\"{i['image_id']}.png\"\n",
    "\n",
    "    height, width = sizes[name]\n",
    "    h_scale = new_height / height\n",
    "    w_scale = new_width / width\n",
    "\n",
    "    x, y, w, h = i['bbox']\n",
    "\n",
    "    x *= w_scale\n",
    "    y *= h_scale\n",
    "    w *= w_scale\n",
    "    h *= h_scale\n",
    "\n",
    "    x_norm = x / new_width\n",
    "    y_norm = y / new_height\n",
    "    w_norm = w / new_width\n",
    "    h_norm = h / new_height\n",
    "\n",
    "    cla = [i['category_id']-1]\n",
    "    #cla[-1] = 1\n",
    "    if name in mydata.keys():\n",
    "        h = mydata[name]\n",
    "        h.append(cla + [x_norm, y_norm, w_norm, h_norm])\n",
    "        mydata[name] = h\n",
    "    else:\n",
    "        mydata[name] = [cla + [x_norm, y_norm, w_norm, h_norm]]\n",
    "    # if img in x_train:\n",
    "    #     h = \n",
    "    # x_train = x_train + [img]\n",
    "    # y_train = y_train +  [cla + [x_norm, y_norm, w_norm, h_norm]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = []\n",
    "y_train = []\n",
    "for x in mydata:\n",
    "    img = Image.open(f\"FaceMask/images/{x}\")\n",
    "    if img.mode == 'RGBA':\n",
    "        t = Image.open(f\"FaceMask/images/{x}\")\n",
    "        img = t.convert('RGB')\n",
    "    x_train.append(img)\n",
    "    y_train.append(mydata[x][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "class CustomDatatset(Dataset):\n",
    "    def __init__(self, data, transform):\n",
    "        self.data = data\n",
    "        self.transforms = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image = self.data[0][index]\n",
    "        tar = self.data[1][index]\n",
    "\n",
    "        #image.resize((256, 256))\n",
    "        \n",
    "        if self.transforms:\n",
    "            image = self.transforms(image)\n",
    "        \n",
    "        # image = image.permute(2, 0, 1)\n",
    "        \n",
    "        return (image, tar)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils\n",
    "import torch.utils.data\n",
    "\n",
    "tran = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize((128, 128)),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "])\n",
    "dat = CustomDatatset(data=(x_train[:-100], torch.tensor(y_train[:-100])), transform=tran)\n",
    "\n",
    "loader = torch.utils.data.DataLoader(dataset=dat, shuffle=True, batch_size=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "model = MyResnet(10)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomLoss, self).__init__()\n",
    "        # Initialize Binary Cross Entropy and Categorical Cross Entropy loss functions\n",
    "        self.bce = nn.SmoothL1Loss(reduction='none')  # For binary classification\n",
    "        self.cce = nn.CrossEntropyLoss(reduction='none')  # For categorical classification\n",
    "\n",
    "    def forward(self, y_true, y_pred):\n",
    "        # Extract parts from y_true and y_pred\n",
    "        loc_true = y_true[:, 1:]\n",
    "        loc_pred = y_pred[:, 3:]\n",
    "        class_true = y_true[:, :1]  # Convert to long for CrossEntropyLoss\n",
    "        class_true = torch.squeeze(class_true)\n",
    "        if class_true.ndim == 0:\n",
    "            # It's a scalar, so convert to a 1D tensor with one element\n",
    "            class_true = torch.tensor([class_true.item()])\n",
    "        class_true = class_true.to(torch.long)\n",
    "        class_pred = y_pred[:, :3]\n",
    "\n",
    "        # Compute binary cross entropy for location\n",
    "        bce = self.bce(loc_pred, loc_true)\n",
    "        bce = bce.mean(dim=1)  # Average over the last dimension (batch size)\n",
    "        \n",
    "        # Compute categorical cross entropy for object class\n",
    "        cce = self.cce(class_pred, class_true)\n",
    "        #cce = cce.mean(dim=-1)  # Average over the last dimension (batch size)\n",
    "\n",
    "        # Combine losses\n",
    "        total_loss = bce.mean() + cce.mean()\n",
    "\n",
    "        return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = CustomLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr=0.0001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tran = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize((128, 128)),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "])\n",
    "val_dat = CustomDatatset(data=(x_train[-100:], torch.tensor(y_train[-100:])), transform=tran)\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dat, shuffle=True, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20: train_loss:0.9355 Val Loss: 0.9690\n",
      "Epoch 2/20: train_loss:0.7630 Val Loss: 0.7386\n",
      "Epoch 3/20: train_loss:0.6788 Val Loss: 0.6789\n",
      "Epoch 4/20: train_loss:0.6315 Val Loss: 0.6672\n",
      "Epoch 5/20: train_loss:0.6132 Val Loss: 0.6653\n",
      "Epoch 6/20: train_loss:0.6023 Val Loss: 0.6650\n",
      "Epoch 7/20: train_loss:0.6125 Val Loss: 0.6656\n",
      "Epoch 8/20: train_loss:0.6058 Val Loss: 0.6643\n",
      "Epoch 9/20: train_loss:0.6192 Val Loss: 0.6648\n",
      "Epoch 10/20: train_loss:0.5893 Val Loss: 0.6645\n",
      "Epoch 11/20: train_loss:0.5919 Val Loss: 0.6634\n",
      "Epoch 12/20: train_loss:0.5978 Val Loss: 0.6625\n",
      "Epoch 13/20: train_loss:0.5952 Val Loss: 0.6619\n",
      "Epoch 14/20: train_loss:0.5903 Val Loss: 0.6630\n",
      "Epoch 15/20: train_loss:0.5889 Val Loss: 0.6622\n",
      "Epoch 16/20: train_loss:0.5842 Val Loss: 0.6605\n",
      "Epoch 17/20: train_loss:0.5735 Val Loss: 0.6615\n",
      "Epoch 18/20: train_loss:0.5903 Val Loss: 0.6615\n",
      "Epoch 19/20: train_loss:0.5883 Val Loss: 0.6603\n",
      "Epoch 20/20: train_loss:0.5883 Val Loss: 0.6588\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "import numpy as np\n",
    "for i in range(20):\n",
    "    model.train()\n",
    "    train_loss = []\n",
    "    val_losses = []\n",
    "    for inputs, targets in loader:\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        loss = criterion(targets, outputs)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss.append(loss.item())\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in val_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            loss = criterion(targets, outputs)\n",
    "\n",
    "            val_losses.append(loss.item())\n",
    "    print(f\"Epoch {i+1}/20: train_loss:{np.mean(train_loss):.4f} Val Loss: {np.mean(val_losses):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = torch.utils.data.DataLoader(dataset=val_dat, shuffle=True, batch_size=1)\n",
    "myimg, myout = next(iter(test_loader))\n",
    "o = model(myimg)\n",
    "bbox = o[0][3:] * 128\n",
    "tr = myout[0][1:] * 128\n",
    "from PIL import ImageDraw\n",
    "myimg = myimg.squeeze(0)  # Remove the batch dimension\n",
    "\n",
    "# Convert the tensor to numpy array\n",
    "myimg = myimg.permute(1, 2, 0).numpy()  # Change shape to [H, W, C]\n",
    "\n",
    "# Normalize if needed\n",
    "if myimg.max() <= 1.0:\n",
    "    myimg = (myimg * 255).astype(np.uint8)  # Convert to [0, 255] if in [0, 1]\n",
    "\n",
    "# Convert to PIL Image\n",
    "ima = Image.fromarray(myimg)\n",
    "draw = ImageDraw.Draw(ima)\n",
    "draw.rectangle([bbox[0], bbox[1], bbox[0]+bbox[2], bbox[1]+bbox[3]], outline='red', width=2)\n",
    "draw.rectangle([tr[0], tr[1], tr[0]+tr[2], tr[1]+tr[3]], outline='black', width=2)\n",
    "ima.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0175, 0.0000, 0.0600, 0.0767]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 128, 128])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myimg[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot select an axis to squeeze out which has size not equal to one",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPIL\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ImageDraw\n\u001b[1;32m----> 2\u001b[0m myimg \u001b[38;5;241m=\u001b[39m \u001b[43mmyimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Remove the batch dimension\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Convert the tensor to numpy array\u001b[39;00m\n\u001b[0;32m      5\u001b[0m myimg \u001b[38;5;241m=\u001b[39m myimg\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mnumpy()  \u001b[38;5;66;03m# Change shape to [H, W, C]\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: cannot select an axis to squeeze out which has size not equal to one"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
